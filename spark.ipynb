{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 检查SpartContext类是否正常"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://JS-NB0318037-20T:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在其他脚本中独立使用sc\n",
    "if False:\n",
    "    from pyspark import SparkConf, SparkContext\n",
    "    conf = SparkConf().setMaster('local').setAppName('MyApp')\n",
    "    sc=SparkContext(conf=conf) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建弹性分布式数据集（RDD）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对某集合、列表进行并行化\n",
    "lines = sc.parallelize(['pandas','i like pandas'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取外部数据\n",
    "lines = sc.textFile('./spark/README.md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD的转化操作transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "转化操作是惰性求值，每次操作对象是RDD的某个元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Apache Spark'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pythonLines = lines.filter(lambda s:'Python' in s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'high-level APIs in Scala, Java, Python, and R, and an optimized engine that'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pythonLines.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[7] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pythonLines.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 high-level APIs in Scala, Java, Python, and R, and an optimized engine that\n"
     ]
    }
   ],
   "source": [
    "print(pythonLines.count(),pythonLines.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "javaLines = lines.filter(lambda s: 'Java' in s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "PJlines = pythonLines.union(javaLines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# better code\n",
    "PJlines=lines.filter(lambda s:('Java' in s)|('Python' in s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD的行动操作action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意：避免传入对象是某个对象的成员或字段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "high-level APIs in Scala, Java, Python, and R, and an optimized engine that\n",
      "## Interactive Python Shell\n",
      "Alternatively, if you prefer Python, you can use the Python shell:\n"
     ]
    }
   ],
   "source": [
    "for line in PJlines.take(10):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 常见转化/行动操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "4\n",
      "9\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "# map\n",
    "nums = sc.parallelize([1,2,3,4])\n",
    "squared = nums.map(lambda x:x**2).collect()\n",
    "for i in squared:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "world\n",
      "hw\n"
     ]
    }
   ],
   "source": [
    "#flatmap\n",
    "lines = sc.parallelize(['hello world', 'hw'])\n",
    "words = lines.flatMap(lambda s:s.split(' '))\n",
    "words.first()\n",
    "for i in words.collect():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#伪集合操作  \n",
    "RDD.distinct() ## 网络数据混洗，开销很大  \n",
    "RDD.union() ##可能包含重复数据  \n",
    "RDD.intersection()  \n",
    "RDD.subtract()  \n",
    "RDD.cartesian()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#行动操作\n",
    "nums = sc.parallelize([1,2,3,4])\n",
    "nums.reduce(lambda x,y:x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 10)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#行动操作--聚合\n",
    "combOp = (lambda par1,par2:(par1[1]+par2[1],par1[0]+par2[0]))\n",
    "seqOp = (lambda acc,num:(acc[0]+num,acc[1]+1))\n",
    "nums.aggregate((0,0),seqOp=seqOp,combOp=combOp)## 求均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'049'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#求各节点的最大值\n",
    "import numpy as np\n",
    "l1 = [1,2,3,4,5,6,7,8,9]\n",
    "rdd1 = sc.parallelize(l1,2)\n",
    "## 由下看出，seqOp和combOp都会吧zerovalue设为起始值\n",
    "rdd1.aggregate(zeroValue=0,seqOp=max,combOp=lambda a,b:(str(a)+str(b))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 其他常用行动操作\n",
    "# nums.count()\n",
    "# nums.collect()\n",
    "# nums.take(3)\n",
    "# nums.top(3)\n",
    "# nums.takeSample(False,2)\n",
    "nums.foreach(lambda x:np.sin(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 持久化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[29] at parallelize at PythonRDD.scala:195"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums.count()\n",
    "nums.takeSample(False,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[29] at parallelize at PythonRDD.scala:195"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#取消持久化\n",
    "nums.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 键值对 pair RDD操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建\n",
    "pairs = lines.map(lambda x: (x.split(' ')[0],x))\n",
    "exa = sc.parallelize({(1,2),(3,4),(3,6)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 3]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[s for s in exa.keys().collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pandas', 'i like pandas']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[s for s in pairs.values().collect()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 转化操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 10]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reduceByKey\n",
    "list(exa.reduceByKey(lambda x,y:x+y).values().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupByKey\n",
    "a,b=exa.groupByKey().values().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 6]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[s for s in a]\n",
    "[s for s in b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 5, 7]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mapValues\n",
    "a=exa.mapValues(lambda x:x+1)\n",
    "[s for s in a.values().collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 3, 1]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sortByKey\n",
    "a=exa.sortByKey(ascending=False)\n",
    "[s for s in a.keys().collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "exa2 = sc.parallelize([(3,90)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# subtractByKey\n",
    "a=exa.subtractByKey(exa2)\n",
    "[x for x in a.keys().collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 3]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# join 内连接\n",
    "a=exa.join(exa2)\n",
    "[x for x in a.keys().collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 90), (6, 90)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rightOtherJoin/leftOtherJoin\n",
    "a=exa.rightOuterJoin(exa2)\n",
    "[x for x in a.values().collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 90), (6, 90)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in a.values().collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, None), (4, 90), (6, 90)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=exa.leftOuterJoin(exa2)\n",
    "[x for x in a.values().collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 其他针对RDD的函数可以用在pairRDD 上\n",
    "a=exa.filter(lambda x:x[1]>5)\n",
    "a.values().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 1), (10, 2)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 聚合操作\n",
    "a=exa.mapValues(lambda x:(x,1)).reduceByKey(lambda x,y:(x[0]+y[0],x[1]+y[1]))#求均值\n",
    "a.values().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a 5\n",
      "b 4\n",
      "c 3\n",
      "d 2\n"
     ]
    }
   ],
   "source": [
    "a=sc.parallelize(list('abcdabcdaaabbc'))\n",
    "b=a.map(lambda x:(x,1)).reduceByKey(lambda x,y:x+y)# 统计字数，自动识别key \n",
    "for k,v in b.collect():\n",
    "    print(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {'a': 5, 'b': 4, 'c': 3, 'd': 2})"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.countByValue()#更快的求字数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combineByKey,函数讲解如下链接\n",
    "# https://blog.csdn.net/jiangpeng59/article/details/52538254\n",
    "exa2 = sc.parallelize([(\"Fred\", 88.0), (\"Fred\", 95.0), (\"Fred\", 91.0), (\"Wilma\", 93.0), (\"Wilma\", 95.0), (\"Wilma\", 98.0)])\n",
    "sumCount = exa2.combineByKey((lambda score:(score,1)),\n",
    "                  (lambda scores,score:(scores[0]+score, scores[1]+1)),\n",
    "                  (lambda scoresA,scoresB:(scoresA[0]+scoresB[0],scoresA[1]+scoresB[1])))\n",
    "a=sumCount.mapValues(lambda v:v[0]/v[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Fred', 91.33333333333333), ('Wilma', 95.33333333333333)]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据分组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupByKey\n",
    "# 接受已成对的数据，接受一个函数，其返回值是用来分组的键\n",
    "if False:\n",
    "    rdd.groupByKey().mapValues(lambda v:v.reduce(func))\n",
    "    等价于\n",
    "    rdd.reduceByKey(lambda v:v.reduce(func))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cogroup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 连接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "exa = sc.parallelize({(1,2),(3,4),(3,6)})\n",
    "exa2 = sc.parallelize({(1,20),(3,40),(3,60),(4,70)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, (2, 20)), (3, (4, 40)), (3, (4, 60)), (3, (6, 40)), (3, (6, 60))]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exa.leftOuterJoin(exa2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, (2, 20)),\n",
       " (3, (4, 40)),\n",
       " (3, (4, 60)),\n",
       " (3, (6, 40)),\n",
       " (3, (6, 60)),\n",
       " (4, (None, 70))]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exa.rightOuterJoin(exa2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "exa = sc.parallelize([(11,3),(21,4),(9,8)])\n",
    "# 按字符串升序排序\n",
    "exa.map(lambda x:(str(x[0]),x[1])).sortByKey().collect().\n",
    "# exa.sortByKey(ascending=True,keyfunc = lambda x:str(x)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 行动操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所有RDD的操作可用在pair RDD上，此外还有特有函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {11: 1, 21: 1, 9: 1})"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exa.countByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{11: 3, 21: 4, 9: 8}"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exa.collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exa.lookup(21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文件读取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文本文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取\n",
    "text = sc.textFile('D:/spark/README.md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['# Apache Spark',\n",
       " '',\n",
       " 'Spark is a fast and general cluster computing system for Big Data. It provides',\n",
       " 'high-level APIs in Scala, Java, Python, and R, and an optimized engine that',\n",
       " 'supports general computation graphs for data analysis. It also supports a',\n",
       " 'rich set of higher-level tools including Spark SQL for SQL and DataFrames,',\n",
       " 'MLlib for machine learning, GraphX for graph processing,',\n",
       " 'and Spark Streaming for stream processing.',\n",
       " '',\n",
       " '<http://spark.apache.org/>',\n",
       " '',\n",
       " '',\n",
       " '## Online Documentation',\n",
       " '',\n",
       " 'You can find the latest Spark documentation, including a programming',\n",
       " 'guide, on the [project web page](http://spark.apache.org/documentation.html).',\n",
       " 'This README file only contains basic setup instructions.',\n",
       " '',\n",
       " '## Building Spark',\n",
       " '',\n",
       " 'Spark is built using [Apache Maven](http://maven.apache.org/).',\n",
       " 'To build Spark and its example programs, run:',\n",
       " '',\n",
       " '    build/mvn -DskipTests clean package',\n",
       " '',\n",
       " '(You do not need to do this if you downloaded a pre-built package.)',\n",
       " '',\n",
       " 'You can build Spark using more than one thread by using the -T option with Maven, see [\"Parallel builds in Maven 3\"](https://cwiki.apache.org/confluence/display/MAVEN/Parallel+builds+in+Maven+3).',\n",
       " 'More detailed documentation is available from the project site, at',\n",
       " '[\"Building Spark\"](http://spark.apache.org/docs/latest/building-spark.html).',\n",
       " '',\n",
       " 'For general development tips, including info on developing Spark using an IDE, see [\"Useful Developer Tools\"](http://spark.apache.org/developer-tools.html).',\n",
       " '',\n",
       " '## Interactive Scala Shell',\n",
       " '',\n",
       " 'The easiest way to start using Spark is through the Scala shell:',\n",
       " '',\n",
       " '    ./bin/spark-shell',\n",
       " '',\n",
       " 'Try the following command, which should return 1000:',\n",
       " '',\n",
       " '    scala> sc.parallelize(1 to 1000).count()',\n",
       " '',\n",
       " '## Interactive Python Shell',\n",
       " '',\n",
       " 'Alternatively, if you prefer Python, you can use the Python shell:',\n",
       " '',\n",
       " '    ./bin/pyspark',\n",
       " '',\n",
       " 'And run the following command, which should also return 1000:',\n",
       " '',\n",
       " '    >>> sc.parallelize(range(1000)).count()',\n",
       " '',\n",
       " '## Example Programs',\n",
       " '',\n",
       " 'Spark also comes with several sample programs in the `examples` directory.',\n",
       " 'To run one of them, use `./bin/run-example <class> [params]`. For example:',\n",
       " '',\n",
       " '    ./bin/run-example SparkPi',\n",
       " '',\n",
       " 'will run the Pi example locally.',\n",
       " '',\n",
       " 'You can set the MASTER environment variable when running examples to submit',\n",
       " 'examples to a cluster. This can be a mesos:// or spark:// URL,',\n",
       " '\"yarn\" to run on YARN, and \"local\" to run',\n",
       " 'locally with one thread, or \"local[N]\" to run locally with N threads. You',\n",
       " 'can also use an abbreviated class name if the class is in the `examples`',\n",
       " 'package. For instance:',\n",
       " '',\n",
       " '    MASTER=spark://host:7077 ./bin/run-example SparkPi',\n",
       " '',\n",
       " 'Many of the example programs print usage help if no params are given.',\n",
       " '',\n",
       " '## Running Tests',\n",
       " '',\n",
       " 'Testing first requires [building Spark](#building-spark). Once Spark is built, tests',\n",
       " 'can be run using:',\n",
       " '',\n",
       " '    ./dev/run-tests',\n",
       " '',\n",
       " 'Please see the guidance on how to',\n",
       " '[run tests for a module, or individual tests](http://spark.apache.org/developer-tools.html#individual-tests).',\n",
       " '',\n",
       " 'There is also a Kubernetes integration test, see resource-managers/kubernetes/integration-tests/README.md',\n",
       " '',\n",
       " '## A Note About Hadoop Versions',\n",
       " '',\n",
       " 'Spark uses the Hadoop core library to talk to HDFS and other Hadoop-supported',\n",
       " 'storage systems. Because the protocols have changed in different versions of',\n",
       " 'Hadoop, you must build Spark against the same version that your cluster runs.',\n",
       " '',\n",
       " 'Please refer to the build documentation at',\n",
       " '[\"Specifying the Hadoop Version and Enabling YARN\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version-and-enabling-yarn)',\n",
       " 'for detailed guidance on building for a particular distribution of Hadoop, including',\n",
       " 'building for particular Hive and Hive Thriftserver distributions.',\n",
       " '',\n",
       " '## Configuration',\n",
       " '',\n",
       " 'Please refer to the [Configuration Guide](http://spark.apache.org/docs/latest/configuration.html)',\n",
       " 'in the online documentation for an overview on how to configure Spark.',\n",
       " '',\n",
       " '## Contributing',\n",
       " '',\n",
       " 'Please review the [Contribution to Spark guide](http://spark.apache.org/contributing.html)',\n",
       " 'for information on how to get started contributing to the project.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存\n",
    "text.saveAsTextFile('D:/spark/new/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存json字符串的文件\n",
    "a=text.filter(lambda s:len(s)>0).map(lambda s:json.dumps(s))\n",
    "# a.collect()\n",
    "a.saveAsTextFile('E:/Learn/pyspark/test1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['# Apache Spark',\n",
       " 'Spark is a fast and general cluster computing system for Big Data. It provides',\n",
       " 'high-level APIs in Scala, Java, Python, and R, and an optimized engine that',\n",
       " 'supports general computation graphs for data analysis. It also supports a',\n",
       " 'rich set of higher-level tools including Spark SQL for SQL and DataFrames,',\n",
       " 'MLlib for machine learning, GraphX for graph processing,',\n",
       " 'and Spark Streaming for stream processing.',\n",
       " '<http://spark.apache.org/>',\n",
       " '## Online Documentation',\n",
       " 'You can find the latest Spark documentation, including a programming',\n",
       " 'guide, on the [project web page](http://spark.apache.org/documentation.html).',\n",
       " 'This README file only contains basic setup instructions.',\n",
       " '## Building Spark',\n",
       " 'Spark is built using [Apache Maven](http://maven.apache.org/).',\n",
       " 'To build Spark and its example programs, run:',\n",
       " '    build/mvn -DskipTests clean package',\n",
       " '(You do not need to do this if you downloaded a pre-built package.)',\n",
       " 'You can build Spark using more than one thread by using the -T option with Maven, see [\"Parallel builds in Maven 3\"](https://cwiki.apache.org/confluence/display/MAVEN/Parallel+builds+in+Maven+3).',\n",
       " 'More detailed documentation is available from the project site, at',\n",
       " '[\"Building Spark\"](http://spark.apache.org/docs/latest/building-spark.html).',\n",
       " 'For general development tips, including info on developing Spark using an IDE, see [\"Useful Developer Tools\"](http://spark.apache.org/developer-tools.html).',\n",
       " '## Interactive Scala Shell',\n",
       " 'The easiest way to start using Spark is through the Scala shell:',\n",
       " '    ./bin/spark-shell',\n",
       " 'Try the following command, which should return 1000:',\n",
       " '    scala> sc.parallelize(1 to 1000).count()',\n",
       " '## Interactive Python Shell',\n",
       " 'Alternatively, if you prefer Python, you can use the Python shell:',\n",
       " '    ./bin/pyspark',\n",
       " 'And run the following command, which should also return 1000:',\n",
       " '    >>> sc.parallelize(range(1000)).count()',\n",
       " '## Example Programs',\n",
       " 'Spark also comes with several sample programs in the `examples` directory.',\n",
       " 'To run one of them, use `./bin/run-example <class> [params]`. For example:',\n",
       " '    ./bin/run-example SparkPi',\n",
       " 'will run the Pi example locally.',\n",
       " 'You can set the MASTER environment variable when running examples to submit',\n",
       " 'examples to a cluster. This can be a mesos:// or spark:// URL,',\n",
       " '\"yarn\" to run on YARN, and \"local\" to run',\n",
       " 'locally with one thread, or \"local[N]\" to run locally with N threads. You',\n",
       " 'can also use an abbreviated class name if the class is in the `examples`',\n",
       " 'package. For instance:',\n",
       " '    MASTER=spark://host:7077 ./bin/run-example SparkPi',\n",
       " 'Many of the example programs print usage help if no params are given.',\n",
       " '## Running Tests',\n",
       " 'Testing first requires [building Spark](#building-spark). Once Spark is built, tests',\n",
       " 'can be run using:',\n",
       " '    ./dev/run-tests',\n",
       " 'Please see the guidance on how to',\n",
       " '[run tests for a module, or individual tests](http://spark.apache.org/developer-tools.html#individual-tests).',\n",
       " 'There is also a Kubernetes integration test, see resource-managers/kubernetes/integration-tests/README.md',\n",
       " '## A Note About Hadoop Versions',\n",
       " 'Spark uses the Hadoop core library to talk to HDFS and other Hadoop-supported',\n",
       " 'storage systems. Because the protocols have changed in different versions of',\n",
       " 'Hadoop, you must build Spark against the same version that your cluster runs.',\n",
       " 'Please refer to the build documentation at',\n",
       " '[\"Specifying the Hadoop Version and Enabling YARN\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version-and-enabling-yarn)',\n",
       " 'for detailed guidance on building for a particular distribution of Hadoop, including',\n",
       " 'building for particular Hive and Hive Thriftserver distributions.',\n",
       " '## Configuration',\n",
       " 'Please refer to the [Configuration Guide](http://spark.apache.org/docs/latest/configuration.html)',\n",
       " 'in the online documentation for an overview on how to configure Spark.',\n",
       " '## Contributing',\n",
       " 'Please review the [Contribution to Spark guide](http://spark.apache.org/contributing.html)',\n",
       " 'for information on how to get started contributing to the project.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读入json字符串的文件\n",
    "import json\n",
    "text = sc.textFile('E:/Learn/pyspark/test1')\n",
    "data = text.map(lambda s:json.loads(s))\n",
    "data.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#写\n",
    "import csv\n",
    "def writeRecords(records):\n",
    "    \"\"\"写出一些CSV记录\"\"\"\n",
    "    output = StringIO.StringIO()\n",
    "    writer = csv.DictWriter(output, fieldnames=[\"name\", \"favoriteAnimal\"])\n",
    "    for record in records:\n",
    "        writer.writerow(record)\n",
    "    return [output.getvalue()]\n",
    "if False:\n",
    "    text.mapPartitions(writeRecords).saveAsTextFile(outputFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#读\n",
    "def loadRecords(fileNameContents):\n",
    "    \"\"\"读取给定文件中的所有记录\"\"\"\n",
    "    input = StringIO.StringIO(fileNameContents[1])\n",
    "    reader = csv.DictReader(input, fieldnames=[\"name\", \"favoriteAnimal\"])\n",
    "    return reader\n",
    "if False:\n",
    "    fullFileData = sc.wholeTextFiles(inputFile).flatMap(loadRecords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SequenceFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#写\n",
    "#SequenceFile只能读写pairRDD\n",
    "text.map(lambda s:(None,s)).saveAsSequenceFile('E:/Learn/pyspark/test2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#读取\n",
    "text = sc.sequenceFile('E:/Learn/pyspark/test2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"# Apache Spark\"',\n",
       " '\"Spark is a fast and general cluster computing system for Big Data. It provides\"',\n",
       " '\"high-level APIs in Scala, Java, Python, and R, and an optimized engine that\"',\n",
       " '\"supports general computation graphs for data analysis. It also supports a\"',\n",
       " '\"rich set of higher-level tools including Spark SQL for SQL and DataFrames,\"',\n",
       " '\"MLlib for machine learning, GraphX for graph processing,\"',\n",
       " '\"and Spark Streaming for stream processing.\"',\n",
       " '\"<http://spark.apache.org/>\"',\n",
       " '\"## Online Documentation\"',\n",
       " '\"You can find the latest Spark documentation, including a programming\"',\n",
       " '\"guide, on the [project web page](http://spark.apache.org/documentation.html).\"',\n",
       " '\"This README file only contains basic setup instructions.\"',\n",
       " '\"## Building Spark\"',\n",
       " '\"Spark is built using [Apache Maven](http://maven.apache.org/).\"',\n",
       " '\"To build Spark and its example programs, run:\"',\n",
       " '\"    build/mvn -DskipTests clean package\"',\n",
       " '\"(You do not need to do this if you downloaded a pre-built package.)\"',\n",
       " '\"You can build Spark using more than one thread by using the -T option with Maven, see [\\\\\"Parallel builds in Maven 3\\\\\"](https://cwiki.apache.org/confluence/display/MAVEN/Parallel+builds+in+Maven+3).\"',\n",
       " '\"More detailed documentation is available from the project site, at\"',\n",
       " '\"[\\\\\"Building Spark\\\\\"](http://spark.apache.org/docs/latest/building-spark.html).\"',\n",
       " '\"For general development tips, including info on developing Spark using an IDE, see [\\\\\"Useful Developer Tools\\\\\"](http://spark.apache.org/developer-tools.html).\"',\n",
       " '\"## Interactive Scala Shell\"',\n",
       " '\"The easiest way to start using Spark is through the Scala shell:\"',\n",
       " '\"    ./bin/spark-shell\"',\n",
       " '\"Try the following command, which should return 1000:\"',\n",
       " '\"    scala> sc.parallelize(1 to 1000).count()\"',\n",
       " '\"## Interactive Python Shell\"',\n",
       " '\"Alternatively, if you prefer Python, you can use the Python shell:\"',\n",
       " '\"    ./bin/pyspark\"',\n",
       " '\"And run the following command, which should also return 1000:\"',\n",
       " '\"    >>> sc.parallelize(range(1000)).count()\"',\n",
       " '\"## Example Programs\"',\n",
       " '\"Spark also comes with several sample programs in the `examples` directory.\"',\n",
       " '\"To run one of them, use `./bin/run-example <class> [params]`. For example:\"',\n",
       " '\"    ./bin/run-example SparkPi\"',\n",
       " '\"will run the Pi example locally.\"',\n",
       " '\"You can set the MASTER environment variable when running examples to submit\"',\n",
       " '\"examples to a cluster. This can be a mesos:// or spark:// URL,\"',\n",
       " '\"\\\\\"yarn\\\\\" to run on YARN, and \\\\\"local\\\\\" to run\"',\n",
       " '\"locally with one thread, or \\\\\"local[N]\\\\\" to run locally with N threads. You\"',\n",
       " '\"can also use an abbreviated class name if the class is in the `examples`\"',\n",
       " '\"package. For instance:\"',\n",
       " '\"    MASTER=spark://host:7077 ./bin/run-example SparkPi\"',\n",
       " '\"Many of the example programs print usage help if no params are given.\"',\n",
       " '\"## Running Tests\"',\n",
       " '\"Testing first requires [building Spark](#building-spark). Once Spark is built, tests\"',\n",
       " '\"can be run using:\"',\n",
       " '\"    ./dev/run-tests\"',\n",
       " '\"Please see the guidance on how to\"',\n",
       " '\"[run tests for a module, or individual tests](http://spark.apache.org/developer-tools.html#individual-tests).\"',\n",
       " '\"There is also a Kubernetes integration test, see resource-managers/kubernetes/integration-tests/README.md\"',\n",
       " '\"## A Note About Hadoop Versions\"',\n",
       " '\"Spark uses the Hadoop core library to talk to HDFS and other Hadoop-supported\"',\n",
       " '\"storage systems. Because the protocols have changed in different versions of\"',\n",
       " '\"Hadoop, you must build Spark against the same version that your cluster runs.\"',\n",
       " '\"Please refer to the build documentation at\"',\n",
       " '\"[\\\\\"Specifying the Hadoop Version and Enabling YARN\\\\\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version-and-enabling-yarn)\"',\n",
       " '\"for detailed guidance on building for a particular distribution of Hadoop, including\"',\n",
       " '\"building for particular Hive and Hive Thriftserver distributions.\"',\n",
       " '\"## Configuration\"',\n",
       " '\"Please refer to the [Configuration Guide](http://spark.apache.org/docs/latest/configuration.html)\"',\n",
       " '\"in the online documentation for an overview on how to configure Spark.\"',\n",
       " '\"## Contributing\"',\n",
       " '\"Please review the [Contribution to Spark guide](http://spark.apache.org/contributing.html)\"',\n",
       " '\"for information on how to get started contributing to the project.\"']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.values().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import HiveContext, Row\n",
    "hiveCtx = HiveContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputFile = hiveCtx.read.json('D:/spark/examples/src/main/resources/employees.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputFile.registerTempTable(\"tweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "topTweets = hiveCtx.sql(\"\"\"SELECT * FROM tweets ORDER BY name LIMIT 3\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(topTweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|   name|salary|\n",
      "+-------+------+\n",
      "|Michael|  3000|\n",
      "|   Andy|  4500|\n",
      "| Justin|  3500|\n",
      "|  Berta|  4000|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputFile.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Michael</td>\n",
       "      <td>3000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Andy</td>\n",
       "      <td>4500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Justin</td>\n",
       "      <td>3500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Berta</td>\n",
       "      <td>4000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      name  salary\n",
       "0  Michael    3000\n",
       "1     Andy    4500\n",
       "2   Justin    3500\n",
       "3    Berta    4000"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputFile.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
